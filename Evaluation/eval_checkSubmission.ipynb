{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/aliabbasi2000/SemEval2025-Task11-Evaluation.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J23PepkHFdbK",
        "outputId": "433f8865-3703-42bd-80f4-68be3f222bb4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SemEval2025-Task11-Evaluation'...\n",
            "remote: Enumerating objects: 173, done.\u001b[K\n",
            "remote: Counting objects: 100% (173/173), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 173 (delta 89), reused 62 (delta 22), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (173/173), 469.46 KiB | 4.65 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SemEval2025-Task11-Evaluation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxxAk_FA8InA",
        "outputId": "9bb9c29e-27b9-4960-ced6-cca6f8511aa5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SemEval2025-Task11-Evaluation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -r requirements.txt\n",
        "!pip install datasets --upgrade"
      ],
      "metadata": {
        "id": "f9wpCVgjF5TD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD5dvr431c2H",
        "outputId": "15c2f820-661f-4a17-80d8-8be7aa258fb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.15.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: tabulate2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.6.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.13 in /usr/local/lib/python3.11/dist-packages (from tabulate2->-r requirements.txt (line 3)) (0.2.13)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "# install dependencies and clone or update the shared task repository\n",
        "!bash settings.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Load the test set for English\n",
        "dataset = load_dataset(\"brighter-dataset/BRIGHTER-emotion-categories\", \"eng\", split=\"test\")\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "# Keep only required columns (assuming these columns exist)\n",
        "required_emotions = ['joy', 'sadness', 'anger', 'fear', 'surprise']\n",
        "df_required = df[['id'] + required_emotions].copy()\n",
        "\n",
        "# Make sure all emotion columns are binary integers\n",
        "df_required[required_emotions] = df_required[required_emotions].astype(int)\n",
        "\n",
        "# Save to CSV\n",
        "df_required.to_csv(\"gold_data/eng.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "s2ePqL4p63Kg"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU8ho5jE1c2S",
        "outputId": "c8b3db8b-f471-4798-e588-37a337d49384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============\n",
            "Checklist:\n",
            "==============\n",
            "+--------------------------+----------+------------------------------------------------------------+\n",
            "| Item                     | Status   | Comment                                                    |\n",
            "+==========================+==========+============================================================+\n",
            "| Submission folder.       | Pass     | Found valid folder: track_a                                |\n",
            "+--------------------------+----------+------------------------------------------------------------+\n",
            "| Submission folder.       | Pass     | Folder name: track_a, starts with \"track_\"                 |\n",
            "+--------------------------+----------+------------------------------------------------------------+\n",
            "| Task name.               | Pass     | Task: A                                                    |\n",
            "+--------------------------+----------+------------------------------------------------------------+\n",
            "| Prediction files.        | Pass     | Found 2 prediction files: .ipynb_checkpoints, pred_eng.csv |\n",
            "+--------------------------+----------+------------------------------------------------------------+\n",
            "| Prediction files.        | Pass     | All prediction files are in the correct format.            |\n",
            "+--------------------------+----------+------------------------------------------------------------+\n",
            "| Submission format check. | Pass     | All checks passed.                                         |\n",
            "+--------------------------+----------+------------------------------------------------------------+\n",
            "\n",
            "Zipping the submission file...\n",
            "Zipped folder: track_a.zip is ready for upload in the codabench submission page.\n",
            "\n",
            "Evaluating prediction files...\n",
            "\n",
            "\n",
            "Files format checked successfully\n",
            "\n",
            "Evaluation scores (micro) for eng track a:\n",
            "Precision: 0.7719\n",
            "Recall: 0.667\n",
            "F1 score: 0.7156\n",
            "\n",
            "Evaluation scores (macro) for eng track a:\n",
            "Precision: 0.765\n",
            "Recall: 0.5943\n",
            "F1 score: 0.6571\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python3 check_submission.py \\\n",
        "  -s track_a \\\n",
        "  --zip_for_submission \\\n",
        "  --evaluate \\\n",
        "  -g gold_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P54CT7Z_1c2U"
      },
      "source": [
        "### Explanation for Participants:\n",
        "\n",
        "- **`-s` (Submission file)**: This is the path to the submission file you're trying to check. It should follow the format `pred_<language_code>_<task>.csv` (e.g., `pred_deu_a.csv` for German, track 'a').\n",
        "\n",
        "- **`-p` (Phase)**: Defines the phase of the competition you're in, either 'dev' for development or 'test' for test submissions.\n",
        "\n",
        "- **`--zip_for_submission`**: This flag will zip the submission file into a `.zip` file after it's validated and/or evaluated. It is helpful when preparing the file for upload to the competition platform.\n",
        "\n",
        "- **`--evaluate`**: This flag instructs the script to evaluate the submission file by comparing it with the provided gold standard file. It will generate evaluation metrics such as precision, recall, and F1 score.\n",
        "\n",
        "- **`-g` (Gold data file)**: This is the path to the gold data file, which contains the correct labels. This is necessary for evaluation, and the script will compare your submission to this file to compute the evaluation metrics.\n",
        "\n",
        "### Additional Notes for Participants:\n",
        "- If you are not ready to evaluate your submission or do not have a gold file, you can omit the `--evaluate` and `-g` arguments.\n",
        "- The zipped file will be created if you use the `--zip_for_submission` flag, and it will be ready for upload to the competition platform.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}